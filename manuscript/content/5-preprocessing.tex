\subsection{Preprocessing}

\subsubsection{File conversion}
% Timing: 1-15 min (10M – 10Gpixels)
% Increasing the number of frames increases the processing time approximately linearly.

AstroCAST is designed to handle a wide range of common input formats, such as .tiff and .czi files, accommodating the diverse nature of imaging data in neuroscience research. One of the key features of astroCAST is its ability to process interleaved datasets. By specifying the \inlinepy{--channels} option, users can automatically split datasets based on imaging channels, which is particularly useful for experiments involving multiple channels interleaved, such as alternating wavelengths.

Moreover, astroCAST supports the subtraction of a static background from the video recordings using the \inlinepy{--subtract-background} option. This feature allows for the provision of a background image or value, which is then subtracted from the entire video. Subtracting for example the dark noise of the camera can significantly enhance the quality of the analysis.

For optimal processing and data management, it is recommended to convert files to the .h5 file format. The .h5 format benefits from smart chunking, enhancing the efficiency of data retrieval and storage. The output configuration can be finely tuned with options such as \inlinepy{--h5-loc} for specifying the dataset location within the .h5 file, \inlinepy{--compression} for selecting a compression algorithm (e.g., `gzip`), and \inlinepy{--dtype} for adjusting the data type if the input differs from the intended storage format.

Chunking is a critical aspect of data management in astroCAST, allowing for the video to be divided into discrete
segments for individual saving and compression. The \inlinepy{--chunks} option enables users to define the chunk size, balancing between retrieval speed and storage efficiency. An appropriately sized chunk can significantly improve processing speed without compromising on efficiency. In cases where the optimal chunk size is uncertain, setting \inlinepy{--chunks} to \inlinepy{None} instructs astroCAST to automatically determine a suitable chunk size.

Lastly, the \inlinepy{--output-path} option directs astroCAST where to save the processed output. While the .h5 format is recommended for its efficiency, astroCAST also supports saving in formats such as .tiff, .tdb, and .avi, providing flexibility to accommodate various research needs and downstream analysis requirements.

\begin{lstlisting}[style=bashStyle]
    # (optional) browse the available flags
    $ astrocast convert-input --help
    # '--config' flag can be ommited to use default settings
    $ astrocast --config "/path/to/config" convert-input "/path/to/file/or/folder"
\end{lstlisting}

We recommend to verify the conversion through a quick visual inspection using the built-in data viewer\fref{2} or an imaging software of your choice (e.g., ImageJ). During this check, ensure that the pixel values are within the expected range (int or float), the image dimensions (width and height) are as anticipated, all frames have been successfully loaded, de-interleaving (if applicable) has been executed correctly, and the dataset name is accurate.

\begin{lstlisting}[style=bashStyle]
    $ astrocast view-data --h5-loc "dataset/name" "/path/to/your/output/file"
\end{lstlisting}

\subsubsection{Motion Correction (optional)}
% Timing: 1-15 min (10M – 10Gpixels)
% Increasing the number of frames increases the processing time approximately linearly.

During imaging, samples will drift or warp which means that the location of the astrocytes might change in the \ac{FOV}. Depending on the amount of movement, this can have detrimental consequences to the analysis. The motion correction module of the \href{https://github.com/flatironinstitute/CaImAn}{CaImAn} package\citep{giovannucci_caiman_2019}, is used in the astroCAST protocol to correct these artifacts. Please see \missing{Supplementary Video S1} for reference. We are including the commonly adjusted parameters here. We refer users to the the \href{https://jnormcorre.readthedocs.io/en/latest/Algo.html#motion-correction-methods}{jnormcorre documentation} for detailed information.

A cornerstone of this feature is the ability to set the maximum allowed rigid shift through the \inlinepy{--max-shifts} parameter. This parameter is critical for accommodating sample motion, ensuring that shifts do not exceed half of the image dimensions, thus balancing between correction effectiveness and computational efficiency. The precision of motion correction is further refined using the \inlinepy{--max-deviation-rigid} option, which limits the deviation of each image patch from the frame's overall rigid shift. This ensures uniformity across the corrected image, enhancing the fidelity of the motion correction process. For iterative refinement of the motion correction, astroCAST employs the \inlinepy{--niter-rig} parameter, allowing up to three iterations by default. This iterative approach enables a more accurate adjustment to the motion correction algorithm, improving the quality of the processed images. To address non-uniform motion across the field of view, astroCAST offers the piecewise-rigid motion correction option, activated by setting \inlinepy{--pw-rigid} to True. This method provides a more nuanced correction by considering different motion patterns within different image segments, leading to superior correction outcomes. Furthermore, the \inlinepy{--gsig-filt} parameter anticipates the half-size of cells in pixels. This information aids in the filtering process, crucial for identifying and correcting motion artifacts accurately.

By carefully adjusting these parameters, researchers can significantly enhance the quality of their imaging data, ensuring that motion artifacts are minimized and that the resultant data is of the highest possible fidelity for subsequent analysis.

\begin{lstlisting}[style=bashStyle]
    # '--config' flag can be ommited to use default settings
    $ astrocast --config "/path/to/config" motion-correction --h5-loc "dataset/name" "/path/to/file"
\end{lstlisting}

\subsubsection{Denoising (optional)}
% Timing: 1-12 h (first time), 1-30 min (inference)

Denoising is an optional but often beneficial preprocessing step in image analysis. The denoising module is based on the architecture suggested in the DeepInterpolation publication\citep{lecoq_removing_2021} and employs a \ac{CNN} designed to clean a target frame.The \ac{CNN} uses adjacent frames as a reference without exposing it to the target frame itself. This approach enables the network to interpolate the desired signal from the contextual frames. Since the noise is stochastic, the network inherently learns to disregard it, effectively isolating and enhancing the signal in the process. We have extended the original approach to support videos of varying dimensions and facilitate straightforward retraining protocols.

Training the denoising model is typically a singular task; once the model is trained, it can be applied to multiple datasets with no further adjustments. The duration of this initial training varies from 1 to 12 hours, contingent on data intricacy and available computational power. Subsequent application of the model to new data, known as inference, is considerably more expedient, usually requiring minutes for each file.

For convenience, we offer a suite of pretrained models tailored to different imaging modalities. A curated list is accessible on our GitHub page under \href{https://github.com/janreising/astroCAST/tree/main/denoiser_models}{denoiser models}, and a comprehensive collection of models can be downloaded via astroCAST.

\begin{lstlisting}[style=bashStyle]
    $ astrocast download-models "/path/to/download/directory/"
\end{lstlisting}

\todo{Jan}{model naming convention}{Explain the naming convention for the model files.}

\paragraph{Training (optional)}

To ensure the robustness and reliability of the denoising process, it is crucial to carefully select the training dataset for the denoiser model. The approach to training should aim to minimize the model's exposure to the data it will denoise, which can enhance the reliability of denoising and prevent overfitting.

One approach is to train the model on a separate data set that will not be used in subsequent analysis. This method is advantageous because it allows the model to learn from a diverse set of data, ideally encompassing all experimental conditions expected in the study. However, it requires having a separate dataset that is representative of the various modalities, which may not always be available.

For smaller datasets or when data is limited, a transformation-based approach can be applied. Training on all experimental data but using rotated frames (90°, 180°, and 270°) ensures that the denoiser is exposed to the inherent noise and variability in the data while mitigating direct exposure to the frames being denoised. This approach is particularly resource-efficient but may result in a less robust model due to the potential predictability of rotated frames.

Each of these methods has its merits and should be chosen based on the specific context and availability of data within a given study.

\begin{lstlisting}[style=bashStyle]
    # explore the possible parameters
    $ astrocast train-denoiser --help
    # train model from config file
    $ astrocast --config config.yaml train-denoiser
\end{lstlisting}

The intricacies of training a custom model are beyond this protocol, and we refer to the original DeepInterpolation
publication\citep{lecoq_removing_2021} and our \href{https://github.com/janreising/astroCAST/blob/8451394bc0d810c4037011f0bcb9cba1f10d1e72/notebooks/examples/train_denoiser_models.ipynb}{example notebook} for users interested in training their own model.

\todo{Jan}{Better notebook}{Maybe it makes sense to write a better tutorial.}

\paragraph{Denoising Data}
Once the neural network has been trained for denoising, it can be employed on new data using the parameters that were set during its training\fref{3}. If utilizing a provided pretrained model, these parameters are inferred from the model file.

For trivial input parameters, such as the model path and output file location, the following flags are used: \inlinepy{--model} to specify the model, \inlinepy{--output-file} to define the output file, \inlinepy{--loc-in} and \inlinepy{--loc-out} to designate the input and output data locations within the HDF5 file structure.

Network architecture parameters are crucial for ensuring that the denoising process is compatible with the data's specific attributes. These parameters need to exactly match the parameters during training. These include \inlinepy{--input-size} to set the dimensions of the input data, \inlinepy{--pre-post-frames} to determine the number of frames before and after the target frame used during denoising, \inlinepy{--gap-frames} to optionally skip frames close to the target frame, and \inlinepy{--normalize} to apply normalization techniques that aid the network in emphasizing important features over noise.

Image parameters cater to the post-processing needs of the denoised data. The \inlinepy{--rescale} parameter is used to reverse the normalization applied during denoising, as subsequent analysis steps often expect the pixel values to be in their original scale. Furthermore, \inlinepy{--padding} adjusts the input data size to match the network's expected input dimensions, ensuring that no data is lost or distorted during the denoising process.

\begin{lstlisting}[style=bashStyle]
    # load a config file with parameters
    $ astrocast --config config.yaml denoise "/path/to/file"
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%
%% Background subtraction

\subsubsection{Background subtraction (optional)}
\label{subsubsec:background}
% Timing: 1-20 min
% Running time will increase linearly with the number of frames and non-linearly with window size.

Background subtraction, while optional, can significantly enhance event detection in fluorescence imaging by mitigating issues related to bright backgrounds, uneven lighting, or pronounced bleaching effects\fref{4}. AstroCAST leverages \ac{RBF} interpolation for background approximation, ensuring accurate background modeling even in scenarios characterized by considerable signal fluctuations or missing data segments.

To facilitate optimal subtraction outcomes without introducing artifacts such as false positives or negatives, default parameters are provided, but fine-tuning may be necessary. An exploratory Jupyter \href{https://github.com/janreising/astroCAST/blob/8451394bc0d810c4037011f0bcb9cba1f10d1e72/notebooks/dev/exploring_delta.ipynb}{notebook} is available for guidance on how parameter adjustments influences the results.

% Intuition behind the process
The process begins with downsizing the video to manage computational load while preserving essential features. Peaks within each pixel's time series are identified and marked as NaN to exclude them from influencing the background model. The RBFInterpolator then estimates the background by interpolating these NaN values across the XYZ dimensions of the video, which is subsequently resized to its original dimensions, and the interpolated background is subtracted.

% Image preparation parameters
For initial image preparation, parameters such as \inlinepy{--scale-factor} for adjusting video resolution, \inlinepy{--blur-sigma} and \inlinepy{--blur-radius} for image blurring, are crucial. These adjustments are preparatory steps aimed at enhancing the effectiveness of the subsequent background subtraction process.

% Peak detection parameters
Peak detection is fine-tuned through parameters like \inlinepy{--prominence}, \inlinepy{--wlen} for window length, \inlinepy{--distance} between peaks, \inlinepy{--width} of peaks, and \inlinepy{--rel-height} defining the peak's cutoff height. These settings ensure precise identification of significant peaks, contributing to the accuracy of the background modeling.

% RBF parameters explanation
The \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.RBFInterpolator.html}{RBF interpolator} constructs a smooth function from scattered data points by combining radial basis functions centered at data locations with a polynomial term, adjusted by coefficients that solve linear equations to fit the data. Interpolation can be tunes with parameters such as \inlinepy{--neighbors} for the interpolation neighborhood, \inlinepy{--rbf-smoothing} for interpolation smoothness, \inlinepy{--rbf-kernel} to choose the kernel type, \inlinepy{--rbf-epsilon}, and \inlinepy{--rbf-degree} for kernel adjustment, facilitating a tailored approach to background modeling.

% Method selection for background subtraction
Background subtraction can be executed using either the Delta F (\inlinepy{--method 'dF'}) method, which directly subtracts the background, or the Delta F over F (\inlinepy{--method 'dFF'}) method, which divides the signal by the background post-subtraction. The choice between these methods depends on the specific requirements of the imaging data and the desired outcome of the subtraction process. However, it's important to consider that background subtraction may not be necessary for all datasets and could potentially introduce biases in event detection. Comparative analysis with and without this preprocessing step is recommended to assess its impact accurately.

\begin{lstlisting}[style=bashStyle]
    $ astrocast subtract-background --h5-loc "mc/ch0" --method "dF" "/path/to/file"
\end{lstlisting}
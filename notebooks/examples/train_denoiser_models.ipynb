{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T16:48:12.773667419Z",
     "start_time": "2023-12-13T16:48:07.730997953Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sys\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "from astrocast.denoising import Network, PyTorchNetwork, SubFrameDataset, SubFrameGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b36036f614441",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63272c0ef73a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T16:48:12.774292413Z",
     "start_time": "2023-12-13T16:48:12.768683900Z"
    }
   },
   "outputs": [],
   "source": [
    "yaml_file = Path(\".\").absolute().parent.parent.joinpath(\"scripts/generate_pretrain_models.yaml\")\n",
    "\n",
    "with open(yaml_file, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "display(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f2424bc0df4ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T16:48:12.806600500Z",
     "start_time": "2023-12-13T16:48:12.768958531Z"
    }
   },
   "outputs": [],
   "source": [
    "params = config[\"param\"]\n",
    "use_pytorch = config[\"use_pytorch\"]\n",
    "use_cpu = config[\"use_cpu\"]\n",
    "\n",
    "root = Path(config[\"root_path\"])\n",
    "model_path = Path(config[\"model_path\"])\n",
    "\n",
    "# change configurations if desired\n",
    "# use_pytorch = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9597e910631f929a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Set up folders for training and saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c117c73b7eb3e96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T16:48:12.807472054Z",
     "start_time": "2023-12-13T16:48:12.792997370Z"
    }
   },
   "outputs": [],
   "source": [
    "# change to your data directory\n",
    "my_data_directory = Path(\"/media/janrei1/data/deep\")\n",
    "\n",
    "assert my_data_directory.exists(), f\"can't find data directory: {my_data_directory}\"\n",
    "\n",
    "root = my_data_directory.joinpath(root)\n",
    "if not root.is_dir():\n",
    "    root.mkdir(parents=True)\n",
    "print(f\"loading data from: {root}\")\n",
    "\n",
    "# Optional: download training files or provide your own\n",
    "# !astrocast download_datasets {{my_data_directory}}\n",
    "\n",
    "model_path = my_data_directory.joinpath(model_path)\n",
    "if not model_path.is_dir():\n",
    "    model_path.mkdir(parents=True)\n",
    "print(f\"saving models to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a619db6e6ed4b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train models\n",
    "## Create all possible configuration combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f56a93eb078782",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T16:48:12.808039837Z",
     "start_time": "2023-12-13T16:48:12.801179285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert single values to lists and keep lists/tuples as-is\n",
    "params = {k: [v] if not isinstance(v, (list, tuple)) else v for k, v in params.items()}\n",
    "\n",
    "# Generate all combinations\n",
    "keys, values = zip(*params.items())\n",
    "combinations = [dict(zip(keys, prod)) for prod in itertools.product(*values)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0093402622b54",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## iterate over combinations and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613437148d9f087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T21:11:58.707429219Z",
     "start_time": "2023-12-13T16:48:12.824714946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop over all combinations\n",
    "for param_set in combinations:\n",
    "    \n",
    "    name_parts = []\n",
    "    for key, value in param_set.items():\n",
    "        \n",
    "        if key in [\"epochs\", \"patience\", \"min_delta\"]:\n",
    "            continue\n",
    "        \n",
    "        if isinstance(value, (list, tuple)):\n",
    "            value_str = '_'.join(map(str, value))\n",
    "        else:\n",
    "            value_str = str(value)\n",
    "        \n",
    "        name_parts.append(f\"{key}_{value_str}\")\n",
    "    name = '-'.join(name_parts)\n",
    "    \n",
    "    for k, v in config[\"data\"].items():\n",
    "        \n",
    "        print(f\"{k}:{name}\")\n",
    "        \n",
    "        save_model_path = model_path.joinpath(f\"{k}_{name}\")\n",
    "        if save_model_path.joinpath(\"model.h5\").is_file():\n",
    "            print(f\"Skipping > model exists: {save_model_path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            input_size = param_set[\"input_size\"]\n",
    "            pre_post_frames = param_set[\"pre_post_frames\"]\n",
    "            gap_frames = param_set[\"gap_frames\"]\n",
    "            train_rotation = param_set[\"train_rotation\"]\n",
    "            \n",
    "            n_stacks, kernel = param_set[\"architecture\"]\n",
    "            epochs = param_set[\"epochs\"]\n",
    "            patience = param_set[\"patience\"]\n",
    "            min_delta = param_set[\"min_delta\"]\n",
    "            \n",
    "            # Trainer\n",
    "            train_str = v[\"train\"]\n",
    "            if \"*\" in train_str:\n",
    "                train_paths = list(root.glob(train_str))\n",
    "            else:\n",
    "                train_paths = root.joinpath(train_str)\n",
    "            \n",
    "            # Validator\n",
    "            val_paths = None\n",
    "            if \"val\" in v:\n",
    "                val_str = v[\"train\"]\n",
    "                if \"*\" in val_str:\n",
    "                    val_paths = list(root.glob(val_str))\n",
    "                else:\n",
    "                    val_paths = root.joinpath(val_str)\n",
    "            \n",
    "            if not use_pytorch:\n",
    "                \n",
    "                train_gen = SubFrameGenerator(paths=train_paths, max_per_file=v[\"max_per_file\"], loc=v[\"loc\"],\n",
    "                                              input_size=input_size,\n",
    "                                              pre_post_frames=pre_post_frames, gap_frames=gap_frames,\n",
    "                                              allowed_rotation=train_rotation,\n",
    "                                              padding=None, batch_size=8, normalize=\"global\", in_memory=False,\n",
    "                                              allowed_flip=[0, 1], shuffle=True)\n",
    "                \n",
    "                # Validator\n",
    "                if val_paths is not None:\n",
    "                    \n",
    "                    val_gen = SubFrameGenerator(\n",
    "                            paths=val_paths, max_per_file=3, loc=v[\"loc\"], input_size=input_size,\n",
    "                            pre_post_frames=pre_post_frames, gap_frames=gap_frames, allowed_rotation=[0],\n",
    "                            padding=None, batch_size=16, normalize=\"global\", in_memory=False,\n",
    "                            cache_results=True,\n",
    "                            allowed_flip=[-1], shuffle=True)\n",
    "                \n",
    "                else:\n",
    "                    val_gen = None\n",
    "                \n",
    "                # Network\n",
    "                \n",
    "                net = Network(train_generator=train_gen, val_generator=val_gen, learning_rate=0.001, decay_rate=0.99,\n",
    "                              pretrained_weights=None,\n",
    "                              n_stacks=n_stacks, kernel=kernel,\n",
    "                              batchNormalize=False, use_cpu=use_cpu)\n",
    "                \n",
    "                net.run(batch_size=1, num_epochs=epochs, patience=patience, min_delta=min_delta,\n",
    "                        save_model=save_model_path)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                train_dataset = SubFrameDataset(paths=train_paths, input_size=input_size, loc=v[\"loc\"],\n",
    "                                                pre_post_frames=pre_post_frames, max_per_file=v[\"max_per_file\"],\n",
    "                                                gap_frames=gap_frames, allowed_rotation=train_rotation, padding=None,\n",
    "                                                normalize=\"global\", in_memory=False, allowed_flip=[0, 1], shuffle=True)\n",
    "                \n",
    "                val_dataset = None\n",
    "                if \"val\" in v:\n",
    "                    val_dataset = SubFrameDataset(paths=val_paths, input_size=input_size, loc=v[\"loc\"],\n",
    "                                                  pre_post_frames=pre_post_frames, max_per_file=3,\n",
    "                                                  gap_frames=gap_frames, allowed_rotation=0, padding=None,\n",
    "                                                  normalize=\"global\", in_memory=False, allowed_flip=-1, shuffle=True)\n",
    "                \n",
    "                net = PyTorchNetwork(train_dataset, val_dataset=val_dataset, batch_size=16, shuffle=True, num_workers=4,\n",
    "                                     learning_rate=0.001, momentum=0.9, decay_rate=0.1, decay_steps=30,\n",
    "                                     n_stacks=n_stacks, kernels=kernel, kernel_size=3, batch_normalize=False,\n",
    "                                     use_cpu=use_cpu)\n",
    "                \n",
    "                net.run(num_epochs=epochs, save_model=save_model_path, patience=patience, min_delta=min_delta)\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            sys.exit(2)\n",
    "        \n",
    "        except Exception as err:\n",
    "            print(f\"Error in {k}:{name}: {err}\")\n",
    "            traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
